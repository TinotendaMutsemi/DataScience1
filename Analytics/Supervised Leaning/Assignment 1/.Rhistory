#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.3, standardize = TRUE)
#plot the coefficients
# plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.3
, standardize = TRUE, type.measure = 'class')
#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.3, standardize = TRUE)
#plot the coefficients
# plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.3
, standardize = TRUE, type.measure = 'class')
#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.3, standardize = TRUE)
#plot the coefficients
# plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.3
, standardize = TRUE, type.measure = 'class')
#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.3, standardize = TRUE)
#plot the coefficients
# plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.3
, standardize = TRUE, type.measure = 'class')
#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.3, standardize = TRUE)
#plot the coefficients
# plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.3
, standardize = TRUE, type.measure = 'class')
#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.8, standardize = TRUE)
#plot the coefficients
# plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.8
, standardize = TRUE, type.measure = 'class')
#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
#get the misclassification rate of the lambda.1se
alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
xlab = 'Alpha', ylab = 'Misclassification rate')
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.3, standardize = TRUE)
#plot the coefficients
# plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.3
, standardize = TRUE, type.measure = 'class')
#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)
#lambda.1se
ipl_en_cv$lambda.1se
#plot the coefficients
plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
#F1 score testing accuracy
x_test <- select(ipl, -Defending.Result, -MatchID) |> slice(-train_index)
x_test <- as.matrix(x_test)
y_test <- ipl$Defending.Result[-train_index]
#predict the test data
y_pred <- predict(ipl_en_cv, newx = x_test, s = ipl_en_cv$lambda.1se, type = 'class')
#confusion matrix
confusion_matrix <- table(y_test, y_pred)
confusion_matrix
#F1 score
F1_score <- 2 * (confusion_matrix[2, 2]) / (2 * (confusion_matrix[2, 2]) + confusion_matrix[1, 2] + confusion_matrix[2, 1])
F1_score
install.packages('ROCR')
#plot the ROC curve
pred <- prediction(y_prob, y_test)
#qtn 2b
library(ROCR)
#get the probabilities of the test data
y_prob <- predict(ipl_en_cv, newx = x_test, s = ipl_en_cv$lambda.1se, type = 'response')
#plot the ROC curve
pred <- prediction(y_prob, y_test)
perf <- performance(pred, 'tpr', 'fpr')
plot(perf, col = 'navy', lwd = 2, main = 'ROC curve')
#calculate the AUC
auc <- performance(pred, 'auc')
auc
plot(perf, col = 'navy', lwd = 2, main = 'ROC curve')
#show point with recall = 0.75
plot(perf, col = 'navy', lwd = 2, main = 'ROC curve')
#add point with recall = 0.75
abline(h = 0.75, col = 'red', lwd = 2)
#qtn 2b
library(ROCR)
#get the probabilities of the test data
y_prob <- predict(ipl_en_cv, newx = x_test, s = ipl_en_cv$lambda.1se, type = 'response')
#plot the ROC curve
pred <- prediction(y_prob, y_test)
perf <- performance(pred, 'tpr', 'fpr')
plot(perf, col = 'navy', lwd = 2, main = 'ROC curve')
#AUC
auc <- performance(pred, 'auc')
#add point with recall = 0.75
abline(h = 0.75, col = 'red', lwd = 2)
abline(v = 0.25, col = 'red', lwd = 2)
#qtn 2b
library(ROCR)
#get the probabilities of the test data
y_prob <- predict(ipl_en_cv, newx = x_test, s = ipl_en_cv$lambda.1se, type = 'response')
#plot the ROC curve
pred <- prediction(y_prob, y_test)
perf <- performance(pred, 'tpr', 'fpr')
plot(perf, col = 'navy', lwd = 2, main = 'ROC curve')
#AUC
auc <- performance(pred, 'auc')
#add point with recall = 0.75
abline(h = 0.75, col = 'red', lwd = 2)
View(perf)
#load the data
parkinsons <- read.csv("Q3dat.csv")
str(parkinsons)
#Qtn 3
#load the data
parkinsons <- read.csv("Q3dat.csv",
colClasses = c(sex = 'factor'))
str(parkinsons)
#lambda.1se
ipl_en_cv$lambda.1se
#Qtn 3
#load the data
parkinsons <- read.csv("Q3dat.csv",
colClasses = c(sex = 'factor'))
str(parkinsons)
#split the data
set.seed(0)
train_index <- sample(nrow(parkinsons), size = 0.8*nrow(parkinsons))
#fit the logistic regression for total_UPDRS using glmnet
x_train <- select(parkinsons, -total_UPDRS, -subject) |> slice(train_index)
#Qtn 3
#load the data
parkinsons <- read.csv("Q3dat.csv",
colClasses = c(sex = 'factor'))
str(parkinsons)
#split the data
set.seed(0)
train_index <- sample(nrow(parkinsons), size = 0.8*nrow(parkinsons))
#fit the logistic regression for total_UPDRS using glmnet
x_train <- select(parkinsons, -total_UPDRS,) |> slice(train_index)
x_train <- as.matrix(x_train)
y_train <- parkinsons$total_UPDRS[train_index]
#fit the elastic net with alpha = 0.5
parkinsons_en <- glmnet(x_train, y_train, family = 'gaussian', alpha = 0.5, standardize = TRUE)
#plot the coefficients
plot(parkinsons_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
#Qtn 3
#load the data
parkinsons <- read.csv("Q3dat.csv",
colClasses = c(sex = 'factor'))
str(parkinsons)
#split the data
set.seed(0)
train_index <- sample(nrow(parkinsons), size = 0.8*nrow(parkinsons))
#fit the logistic regression for total_UPDRS using glmnet
x_train <- select(parkinsons, -total_UPDRS,) |> slice(train_index)
x_train <- as.matrix(x_train)
y_train <- parkinsons$total_UPDRS[train_index]
#fit the elastic net with alpha = 0.5
parkinsons_en <- glmnet(x_train, y_train, family = 'gaussian', alpha = 0.3, standardize = TRUE)
#plot the coefficients
plot(parkinsons_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
#Qtn 3
#load the data
parkinsons <- read.csv("Q3dat.csv",
colClasses = c(sex = 'factor'))
str(parkinsons)
#split the data
set.seed(0)
train_index <- sample(nrow(parkinsons), size = 0.8*nrow(parkinsons))
#fit the logistic regression for total_UPDRS using glmnet
x_train <- select(parkinsons, -total_UPDRS,) |> slice(train_index)
x_train <- as.matrix(x_train)
y_train <- parkinsons$total_UPDRS[train_index]
#fit the elastic net with alpha = 0.5
parkinsons_en <- glmnet(x_train, y_train, family = 'gaussian', alpha = 1, standardize = TRUE)
#plot the coefficients
plot(parkinsons_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
#Qtn 3
#load the data
parkinsons <- read.csv("Q3dat.csv",
colClasses = c(sex = 'factor'))
str(parkinsons)
#fitting vannila logistic regression
parkinsons_lr <- glm(total_UPDRS ~ ., data = parkinsons, subset = train_index, family = 'gaussian')
parkinsons_lr |> tidy() |> kable(digits = 2, caption = 'Saturated logistic regression model fitted to UPDRS data')
#Qtn 3
#load the data
parkinsons <- read.csv("Q3dat.csv",
colClasses = c(sex = 'factor'))
str(parkinsons)
#fitting vannila logistic regression
parkinsons_lr <- glm(total_UPDRS ~ ., data = parkinsons, subset = train_index, family = 'gaussian')
parkinsons_lr |> tidy() |> kable(digits = 2, caption = 'Saturated logistic regression model fitted to UPDRS data')
#split the data
set.seed(0)
train_index <- sample(nrow(parkinsons), size = 0.8*nrow(parkinsons))
#fit the logistic regression for total_UPDRS using glmnet
x_train <- select(parkinsons, -total_UPDRS,) |> slice(train_index)
x_train <- as.matrix(x_train)
y_train <- parkinsons$total_UPDRS[train_index]
#fit the elastic net with alpha = 0.5
parkinsons_en <- glmnet(x_train, y_train, family = 'gaussian', alpha = 1, standardize = TRUE)
#plot the coefficients
plot(parkinsons_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
#plot the coefficients
plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
parkinsons_en_cv <- cv.glmnet(x_train, y_train, family = 'gaussian', alpha = 1
, standardize = TRUE, type.measure = 'mse')
#plot the cross validation error
plot(parkinsons_en_cv, xlab = 'Log(lambda)', ylab = 'Mean Squared Error', col = 'navy', lwd = 2)
#lambda.1se
parkinsons_en_cv$lambda.1se
#plot the cross validation error
plot(parkinsons_en_cv, xlab = 'Log(lambda)', ylab = 'Mean Squared Error', col = 'navy', lwd = 2)
#add the lambda.1se and lambda.min points to the plot
abline(v = log(parkinsons_en_cv$lambda.1se), col = 'red', lwd = 2)
plot(parkinsons_en_cv, xlab = 'Log(lambda)', ylab = 'Mean Squared Error', col = 'navy', lwd = 2)
#add the lambda.1se and lambda.min points to the plot
abline(v = log(parkinsons_en_cv$lambda.1se), col = 'red', lwd = 2)
abline(v = log(parkinsons_en_cv$lambda.min), col = 'green', lwd = 2)
#show coefficients of the lambda.1se
coef(parkinsons_en_cv, s = parkinsons_en_cv$lambda.1se)
#show mean squared error of the lambda.1se
parkinsons_en_cv$cvm[which(parkinsons_en_cv$lambda == parkinsons_en_cv$lambda.1se)]
#show coefficients of the lambda.1se
coef(parkinsons_en_cv, s = parkinsons_en_cv$lambda.1se)
corrplot(cor(x_train))
#Qtn 3
library(corrplot)
corrplot(cor(x_train))
parkinsons_lr |> tidy() |> kable(digits = 2, caption = 'Saturated logistic regression model fitted to UPDRS data')
x_train_cor <- select(parkinsons, -sex,) |> slice(train_index)
corrplot(cor(x_train_cor), method = 'number', type = 'upper')
#add the lambda.1se and lambda.min points to the plot
abline(v = log(parkinsons_en_cv$lambda.1se), col = 'red', lwd = 2)
plot(parkinsons_en_cv, xlab = 'Log(lambda)', ylab = 'Mean Squared Error', col = 'navy', lwd = 2)
#add the lambda.1se and lambda.min points to the plot
abline(v = log(parkinsons_en_cv$lambda.1se), col = 'red', lwd = 2)
abline(v = log(parkinsons_en_cv$lambda.min), col = 'green', lwd = 2)
#predict the test data
y_pred <- predict(parkinsons_en_cv, newx = x_test, s = parkinsons_en_cv$lambda.1se)
x_train_cor <- select(parkinsons, -sex,) |> slice(train_index)
corrplot(cor(x_train_cor))
x_test <- select(parkinsons, -total_UPDRS,) |> slice(-train_index)
#show coefficients of the lambda.1se
coef(parkinsons_en_cv, s = parkinsons_en_cv$lambda.1se)
#show mean squared error of the lambda.1se
parkinsons_en_cv$cvm[which(parkinsons_en_cv$lambda == parkinsons_en_cv$lambda.1se)]
#predict the test data
y_pred <- predict(parkinsons_en_cv, newx = x_test, s = parkinsons_en_cv$lambda.1se, type = 'response')
x_test <- as.matrix(x_test)
#predict the test data
y_pred <- predict(parkinsons_en_cv, newx = x_test, s = parkinsons_en_cv$lambda.1se, type = 'response')
#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)
#show mean squared error of the lambda.1se
parkinsons_en_cv$cvm[which(parkinsons_en_cv$lambda == parkinsons_en_cv$lambda.1se)]
#plot the coefficients
plot(parkinsons_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
#plot the cross validation error
plot(parkinsons_en_cv, xlab = 'Log(lambda)', ylab = 'Mean Squared Error', col = 'navy', lwd = 2)
#add the lambda.1se and lambda.min points to the plot
abline(v = log(parkinsons_en_cv$lambda.1se), col = 'red', lwd = 2)
plot.new
#plot the cross validation error
plot(parkinsons_en_cv, xlab = 'Log(lambda)', ylab = 'Mean Squared Error', col = 'navy', lwd = 2)
#add the lambda.1se and lambda.min points to the plot
abline(v = log(parkinsons_en_cv$lambda.1se), col = 'red', lwd = 2, add = TRUE)
#plot the coefficients
plot(parkinsons_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')
?train
library(caret)
install.packages('caret')
#show coefficients of the lambda.1se
coef(parkinsons_en_cv, s = parkinsons_en_cv$lambda.1se)
library(caret)
#KNN
#fit the model using knnreg
knn_grid <- expand.grid(k = 5:20)
knn_control <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
knn_model <- train(total_UPDRS ~ age + sex + Jitter.Abs + Shimmer.APQ3 + Shimmer.DDA + NHR + HNR + RPDE + DFA + PPE,
data = parkinsons,
subset = train_index,
method = 'knn',
trControl = knn_control,
tuneGrid = knn_grid)
library(caret)
#KNN
#fit the model using knnreg
knn_grid <- expand.grid(k = 5:20)
knn_control <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
knn_model <- train(total_UPDRS ~ age + sex + Jitter.Abs. + Shimmer.APQ3 + Shimmer.DDA + NHR + HNR + RPDE + DFA + PPE,
data = parkinsons,
subset = train_index,
method = 'knn',
trControl = knn_control,
tuneGrid = knn_grid)
#plot the model
plot(knn_model)
library(caret)
#KNN
#fit the model using knnreg
knn_grid <- expand.grid(k = 1:20)
knn_control <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
knn_model <- train(total_UPDRS ~ age + sex + Jitter.Abs. + Shimmer.APQ3 + Shimmer.DDA + NHR + HNR + RPDE + DFA + PPE,
data = parkinsons,
subset = train_index,
method = 'knn',
trControl = knn_control,
tuneGrid = knn_grid)
#plot the model
plot(knn_model)
#select k = 3 as the best k
knn_model$bestTune
#predict the test data using the best model
y_pred <- predict(knn_model, newdata = parkinsons[-train_index,])
#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)
#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)
#mse of the best model
knn_model$results[which(knn_model$bestTune == knn_model$bestTune$k),]
#plot the model
plot(knn_model)
#k = 4 as shown by the plot
knn_model$bestTune
#mse of the best model
knn_model$results[which(knn_model$bestTune == knn_model$bestTune$k),]
#predict the test data using the best model
y_pred <- predict(knn_model, newdata = parkinsons[-train_index,])
#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)
install.packages('randomForest')
#fit the model using random forest
library(randomForest)
rf_grid <- expand.grid(mtry = 1:10)
rf_control <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
rf_model <- train(total_UPDRS ~ age,
data = parkinsons,
subset = train_index,
method = 'rf',
trControl = rf_control,
tuneGrid = rf_grid)
rf_300 <- randomForest(total_UPDRS ~ .,
data = parkinsons,
subset = train_index,
ntree = 300,
importance = TRUE,
na.action = na.exclude)
#plot the model
plot(rf_300)
rf_300$importance
#predict the test data using the best model
y_pred <- predict(rf_300, newdata = parkinsons[-train_index,])
#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)
#select the best model
rf_300
#variable importance
varImpPlot(rf_300)
#variable importance table
importance(rf_300)
#variable importance
varImpPlot(rf_300)
#partial dependence plot
partialPlot(rf_300, parkinsons, age, main = 'Partial Dependence of Age on UPDRS')
par(mar = c(5, 6, 4, 1) + 0.1)
rf_300_imp <- randomForest::importance(rf_300, type = 1)
rf_300_imp <- rf_300_imp[order(rf_300_imp[, 1], decreasing = TRUE), ]
barplot(rf_300_imp[, 1], las = 2, cex.names = 0.8, main = 'Variable Importance', col = 'navy')
par(mar = c(5, 6, 4, 1) + 0.1)
rf_300_imp <- randomForest::importance(rf_300, type = 1)
rf_300_imp <- rf_300_imp[order(rf_300_imp[, 1], decreasing = TRUE), ]
barplot(rf_300_imp, horiz = TRUE, col = 'navy', las = 1, main = 'Variable Importance')
rf_300_imp <- rf_300_imp[order(rf_300_imp[, 1], decreasing = FALSE), ]
par(mar = c(5, 6, 4, 1) + 0.1)
rf_300_imp <- randomForest::importance(rf_300, type = 1)
rf_300_imp <- rf_300_imp[order(rf_300_imp[, 1], decreasing = FALSE), ]
barplot(rf_300_imp, horiz = TRUE, col = 'navy', las = 1, main = 'Variable Importance')
install.packages('ranger')
rf_grid <- expand.grid(mtry = 2: ncol(parkinsons) - 1,
splitrule = c('variance', 'extratrees'),
min.node.size = c(1, 5, 10, 15, 20))
rf_control <- trainControl(method = 'repeatedcv')
rf_model <- train(total_UPDRS ~ .,
data = parkinsons,
subset = train_index,
method = 'ranger',
trControl = rf_control,
tuneGrid = rf_grid)
