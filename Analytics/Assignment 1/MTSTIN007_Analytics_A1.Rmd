```{r}
#Q1
#load data
mealies <- read.csv("Q1dat.csv")

str(mealies)

#convert data types 
mealies$pests <- as.factor(mealies$pests)
mealies$quality <- as.factor(mealies$quality)
nrow(mealies)
```

```{r}
set.seed(1)
#split data into training and test 80/20 split
train_index <- sample(nrow(mealies), 0.8*nrow(mealies))

y_train <- mealies$quality[train_index]

library(tree)
#check ?text.tree to see the any infor of the nodes
#summary(tree_name_here) to see the tree details eg dev, 
#print the tree to see the tree info
mealies_big_tree <- tree(quality ~ . , data = mealies, subset = train_index, 
                  split = 'deviance',
                  control = tree.control(nobs=length(train_index), minsize = 2, mindev = 0))
plot.new
plot(mealies_big_tree)
text(mealies_big_tree, pretty = 0)

#Prediction accuracy
y_true  <-  mealies$quality[-train_index]
y_pred <- predict(mealies_big_tree, newdata = mealies[-train_index,], type = "class")

#misclassification rate
mean(y_true != y_pred)

#confusion matrix
table(y_pred, y_true)

```

```{r}
#Q1b
set.seed(0)
mealies_cv <- cv.tree(mealies_big_tree, 
                      FUN = prune.misclass,
                      K = 10) #K is the number of folds

mealies_cv
plot.new
plot(mealies_cv
     $size, mealies_cv$dev, type = 'o',
     pch = 16, col = 'navy', lwd = 2,
     xlab = "Num of terminal nodes", ylab = "CV error")

#remove -inf for 0 penality
mealies_cv$k[1] <- 0
alpha <- round(mealies_cv$k, 1) #the penality 
axis(3, at = mealies_cv$size, lab = alpha, cex.axis = 0.8)
mtext(expression(alpha), 3, line = 2.5, cex = 1.2)

min_cv_error_tree_size <- mealies_cv$size[which.min(mealies_cv$dev)]
abline(v = min_cv_error_tree_size, lty = 2, col = 'red')

chosen_cv_error_tree_size <- mealies_cv$size[which(mealies_cv$k == 9)]
abline(v = chosen_cv_error_tree_size, lty = 2, col = 'green')

#prune the tree
pruned_mealies_tree <- prune.misclass(mealies_big_tree, best = chosen_cv_error_tree_size)

plot.new
plot(pruned_mealies_tree)
text(pruned_mealies_tree, pretty = 0)

#Prediction accuracy
y_true  <-  mealies$quality[-train_index]
y_pred <- predict(pruned_mealies_tree, newdata = mealies[-train_index,], type = "class")

#misclassification rate
mean(y_true != y_pred)

#confusion matrix
table(y_pred, y_true)

```


```{r}
#Q1c
library(maps)
library(RColorBrewer)

plot.new
plot(mealies$latitude, mealies$longitude,
     pch = 16, cex = 0.5, bty = 'L',
     xlab = 'Lattitude', ylab = 'Longitude', col = mealies$quality)

legend('topright', legend = levels(mealies$quality), pch = 16, col = 1:4)


```


```{r}
#rotate location feature space over thetas
rotated_mealies <- mealies
theta <- seq(0, pi/2, by = pi/100)
misclass <- rep(-1, length(theta))
for (i in 1:length(theta)){

  rotated_mealies$latitude <- cos(theta[i]) * mealies$latitude - sin(theta[i]) *   mealies$longitude
  rotated_mealies$longitude <- sin(theta[i]) * mealies$latitude + cos(theta[i]) * mealies$longitude
  
  #apply 10 fold cross validation to the rotated data
  rotated_mealies_big_tree <- tree(quality ~ . , data = rotated_mealies, subset = train_index, 
                    split = 'deviance',
                    control = tree.control(nobs=length(train_index), minsize = 2, mindev = 0))
  
  #cross validation
  rotated_mealies_cv <- cv.tree(rotated_mealies_big_tree, 
                        FUN = prune.misclass,
                        K = 10) #K is the number of folds
 
  #remove -inf for 0 penality
  rotated_mealies_cv$k[1] <- 0
  alpha <- round(rotated_mealies_cv$k, 1) #the penality 
  min_cv_error_tree_size <- rotated_mealies_cv$size[which.min(rotated_mealies_cv$dev)]
  pruned_rotated_mealies_tree <- prune.misclass(rotated_mealies_big_tree, best = 4)#set best = 4 and see misclass rates
  

  #Prediction accuracy
  y_true  <-  rotated_mealies$quality[-train_index]
  y_pred <- predict(pruned_rotated_mealies_tree, newdata = rotated_mealies[-train_index,], type = "class")
  
  #misclassification rate
  misclass[i] <- mean(y_true != y_pred)

}

#get the best theta
best_theta <- theta[which.min(misclass)]
best_theta
```

```{r}


#plot misclss rate against theta
plot.new
plot(theta, misclass, type = 'l', col = 'navy', lwd = 2,
     xlab = 'Theta', ylab = 'Misclassification rate')
```

```{r}
theta_i <- 0.9738937 #4 trees
rotated_mealies$latitude <- cos(theta_i) * mealies$latitude - sin(theta_i) *   mealies$longitude
  rotated_mealies$longitude <- sin(theta_i) * mealies$latitude + cos(theta_i) * mealies$longitude
  
#apply 10 fold cross validation to the rotated data
rotated_mealies_big_tree <- tree(quality ~ . , data = rotated_mealies, subset = train_index, 
                  split = 'deviance',
                  control = tree.control(nobs=length(train_index), minsize = 2, mindev = 0))

#cross validation
rotated_mealies_cv <- cv.tree(rotated_mealies_big_tree, 
                      FUN = prune.misclass,
                      K = 10) #K is the number of folds

#remove -inf for 0 penality
rotated_mealies_cv$k[1] <- 0
alpha <- round(rotated_mealies_cv$k, 1) #the penality 
min_cv_error_tree_size <- rotated_mealies_cv$size[which.min(rotated_mealies_cv$dev)]
pruned_rotated_mealies_tree <- prune.misclass(rotated_mealies_big_tree, best = min_cv_error_tree_size)#set best = 4 and see misclass rates


#Prediction accuracy
y_true  <-  rotated_mealies$quality[-train_index]
y_pred <- predict(pruned_rotated_mealies_tree, newdata = rotated_mealies[-train_index,], type = "class")

#misclassification rate
mean(y_true != y_pred)


```

```{r}
#Qtn 2
#a
#remove match id from the regression,
#statndadize data in the regression
ipl <- read.csv("Q2dat.csv",
                    colClasses = c(Defending.Result = 'factor',
                                   Time = 'factor',
                                   Defending.Toss = 'factor',
                                   Defending.Team = 'factor',
                                   Chasing.Team = 'factor',
                                   Defending.Stadium = 'factor'))

str(ipl)


```
```{r}
plot(ipl$First.Inn.Score, ipl$Bowl2.Strength,
     col = ifelse(ipl$Defending.Result == 1, 'blue', 'red'),
     pch = ifelse(ipl$Defending.Result == 1, 3, 1),
     xlab = 'First Inn Score', ylab = 'Bowl2 Strength')

legend('topright', c('Win', 'Loss'),
       col = c('blue', 'red'),
       pch = c(3, 1))

```


```{r}
library(kableExtra)
library(broom)
set.seed(0)
train_index <- sample(nrow(ipl), size = 0.8*nrow(ipl))

#checking the prevalance rate
y_train <- ipl$Defending.Result[train_index]
mean(as.numeric(y_train) - 1)

#fitting vannila logistic regression
ipl_lr <- glm(Defending.Result ~ ., data = ipl, subset = train_index, family = 'binomial')

ipl_lr |> tidy() |> kable(digits = 2, caption = 'Saturated logistic regression model fitted to IPL data')

```




```{r}
library(glmnet)
library(plotmo)
library(dplyr)

x_train <- select(ipl, -Defending.Result, -MatchID) |> slice(train_index)
x_train <- as.matrix(x_train)
y_train <- ipl$Defending.Result[train_index]
```

```{r}

#range over which alpha is to be used
alpha_seq <- seq(0, 1, by = 0.1)
alpha_plot <- rep(0, length(alpha_seq))
for (i in 1:length(alpha_seq)){
  ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = alpha_seq[i], standardize = TRUE, type.measure = 'class')
  #get the misclassification rate of the lambda.1se
  alpha_plot[i] <- ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]
}
#print the lambda.1se
# coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)

plot(alpha_seq, alpha_plot, type = 'l', col = 'navy', lwd = 2,
     xlab = 'Alpha', ylab = 'Misclassification rate')

#alpha 0.3 is the best
```



```{r}
#fitting the elastic net with alpha = 0.3
ipl_en <- glmnet(x_train, y_train, family = 'binomial', alpha = 0.3, standardize = TRUE)
#plot the coefficients
plot(ipl_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')

ipl_en_cv <- cv.glmnet(x_train, y_train, family = 'binomial', alpha = 0.3
                       , standardize = TRUE, type.measure = 'class')

#lambda.1se
ipl_en_cv$lambda.1se


#misclassification rate
ipl_en_cv$cvm[which(ipl_en_cv$lambda == ipl_en_cv$lambda.1se)]

#print the coefficients
coef(ipl_en_cv, s = ipl_en_cv$lambda.1se)

```


```{r}
#F1 score testing accuracy
x_test <- select(ipl, -Defending.Result, -MatchID) |> slice(-train_index)
x_test <- as.matrix(x_test)
y_test <- ipl$Defending.Result[-train_index]

#predict the test data
y_pred <- predict(ipl_en_cv, newx = x_test, s = ipl_en_cv$lambda.1se, type = 'class')
#confusion matrix
confusion_matrix <- table(y_test, y_pred)
confusion_matrix

#F1 score
F1_score <- 2 * (confusion_matrix[2, 2]) / (2 * (confusion_matrix[2, 2]) + confusion_matrix[1, 2] + confusion_matrix[2, 1])
F1_score

#the test accuracy is 0.43 showing that the model is not good because the F1 score is less than 0.5

```
```{r}
#qtn 2b
library(ROCR)


#get the probabilities of the test data
y_prob <- predict(ipl_en_cv, newx = x_test, s = ipl_en_cv$lambda.1se, type = 'response')

#plot the ROC curve
pred <- prediction(y_prob, y_test)
perf <- performance(pred, 'tpr', 'fpr')
plot(perf, col = 'navy', lwd = 2, main = 'ROC curve')

#AUC
auc <- performance(pred, 'auc')
#add point with recall = 0.75
abline(h = 0.75, col = 'red', lwd = 2)





```






```{r}
#Qtn 3
library(corrplot)
library(kableExtra)
library(broom)
library(dplyr)
#load the data
parkinsons <- read.csv("Q3dat.csv",
                       colClasses = c(sex = 'factor'))

str(parkinsons)

#split the data
set.seed(0)
train_index <- sample(nrow(parkinsons), size = 0.8*nrow(parkinsons))

#EDA
#fitting vannila logistic regression
parkinsons_lr <- glm(total_UPDRS ~ ., data = parkinsons, subset = train_index, family = 'gaussian')


parkinsons_lr |> tidy() |> kable(digits = 2, caption = 'Saturated logistic regression model fitted to UPDRS data')


x_train_cor <- select(parkinsons, -sex,) |> slice(train_index)
corrplot(cor(x_train_cor))

#Set data for train and test
x_train <- select(parkinsons, -total_UPDRS,) |> slice(train_index)
x_test <- select(parkinsons, -total_UPDRS,) |> slice(-train_index)
x_train <- as.matrix(x_train)
x_test <- as.matrix(x_test)
y_train <- parkinsons$total_UPDRS[train_index]

```




```{r}

#split the data
set.seed(0)
train_index <- sample(nrow(parkinsons), size = 0.8*nrow(parkinsons))

#fit the logistic regression for total_UPDRS using glmnet

#fit the elastic net with alpha = 0.5
parkinsons_en <- glmnet(x_train, y_train, family = 'gaussian', alpha = 1, standardize = TRUE)

#plot the coefficients
plot(parkinsons_en, xvar = 'lambda', label = TRUE, xlab = 'Lambda', ylab = 'Coefficients')

parkinsons_en_cv <- cv.glmnet(x_train, y_train, family = 'gaussian', alpha = 1
                       , standardize = TRUE, type.measure = 'mse')

plot.new
#plot the cross validation error
plot(parkinsons_en_cv, xlab = 'Log(lambda)', ylab = 'Mean Squared Error', col = 'navy', lwd = 2)

#add the lambda.1se and lambda.min points to the plot 
abline(v = log(parkinsons_en_cv$lambda.1se), col = 'red', lwd = 2)
abline(v = log(parkinsons_en_cv$lambda.min), col = 'green', lwd = 2)

#show coefficients of the lambda.1se
coef(parkinsons_en_cv, s = parkinsons_en_cv$lambda.1se)

#show mean squared error of the lambda.1se
parkinsons_en_cv$cvm[which(parkinsons_en_cv$lambda == parkinsons_en_cv$lambda.1se)]

#predict the test data
y_pred <- predict(parkinsons_en_cv, newx = x_test, s = parkinsons_en_cv$lambda.1se, type = 'response')

#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)

```

```{r}

library(caret)

#KNN
#fit the model using knnreg
knn_grid <- expand.grid(k = 1:20)
knn_control <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
knn_model <- train(total_UPDRS ~ age + sex + Jitter.Abs. + Shimmer.APQ3 + Shimmer.DDA + NHR + HNR + RPDE + DFA + PPE,
                   data = parkinsons,
                   subset = train_index,
                   method = 'knn', 
                   trControl = knn_control, 
                   tuneGrid = knn_grid)

#plot the model
plot(knn_model)

#k = 4 as shown by the plot
knn_model$bestTune
#mse of the best model
knn_model$results[which(knn_model$bestTune == knn_model$bestTune$k),]

#predict the test data using the best model
y_pred <- predict(knn_model, newdata = parkinsons[-train_index,])

#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)


```

```{r}

#fit the model using random forest
library(randomForest)

rf_300 <- randomForest(total_UPDRS ~ .,
                       data = parkinsons,
                       subset = train_index,
                       ntree = 300,
                       importance = TRUE,
                       na.action = na.exclude)


#plot the model
plot(rf_300)

#select the best model
rf_300

#predict the test data using the best model
y_pred <- predict(rf_300, newdata = parkinsons[-train_index,])

#mean squared error
mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)


#variable importance table
importance(rf_300)

#variable importance
varImpPlot(rf_300)

par(mar = c(5, 6, 4, 1) + 0.1)
rf_300_imp <- randomForest::importance(rf_300, type = 1)
rf_300_imp <- rf_300_imp[order(rf_300_imp[, 1], decreasing = FALSE), ]
barplot(rf_300_imp, horiz = TRUE, col = 'navy', las = 1, main = 'Variable Importance')

```

```{r}
#use ranger and caret to fit the model
library(caret)
library(ranger)

rf_grid <- expand.grid(mtry = 2: ncol(parkinsons) - 1,
                       splitrule = c('variance', 'extratrees'),
                       min.node.size = c(1, 5, 10, 15, 20))
rf_control <- trainControl(method = 'repeatedcv')
rf_model_grid_search <- train(total_UPDRS ~ .,
                  data = parkinsons,
                  subset = train_index,
                  method = 'ranger',
                  trControl = rf_control,
                  tuneGrid = rf_grid)

rf_model_grid_search$finalModel

plot(rf_model_grid_search)

plot(varImp(rf_model_grid_search))

```


```{r}

#fit the model using xgbTree

library(caret)
xgb_grid <- expand.grid(nrounds = 100,
                        max_depth = 1:10,
                        eta = c(0.01, 0.1, 0.3),
                        gamma = 0,
                        colsample_bytree = 1,
                        min_child_weight = 1,
                        subsample = 1)

xgb_control <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)

xgb_model <- train(total_UPDRS ~ .,
                   data = parkinsons,
                   subset = train_index,
                   method = 'xgbTree',
                   trControl = xgb_control,
                   tuneGrid = xgb_grid)

#save the model
save(xgb_model, file = 'xgb_model.RData')

#load the model
load('xgb_model.RData')

xgb_model$bestTune

plot(varImp(xgb_model))

plot(xgb_model)

xgb_model$results[which(xgb_model$bestTune == xgb_model$bestTune$nrounds),]

y_pred <- predict(xgb_model, newdata = parkinsons[-train_index,])

mean((y_pred - parkinsons$total_UPDRS[-train_index])^2)


```

```{r}
```

























